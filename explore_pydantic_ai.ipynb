{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we have to make \n",
    "\n",
    "user will give you a situation. \n",
    "first agent will ask the additional questions if necessary \n",
    "\n",
    "second and third agent will provide pros and cons of the situation. If it requires any tool calls have a web search and simple calculator tool powered by python make both tools availalbe to this agent. \n",
    "\n",
    "forth agent: \n",
    "will provide the final conclusion if user should take that decision or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for internet search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.tools import Tool\n",
    "\n",
    "def search_internet(query: str) -> list[dict]: \n",
    "    \"\"\"Searches the web for given query and returns top 5 results\n",
    "\n",
    "    Args:\n",
    "        query (str): Query to search\n",
    "\n",
    "    Returns:\n",
    "        list: List of top 5 results\n",
    "    \"\"\"\n",
    "    results = DDGS().text(query, max_results=5)\n",
    "    return results\n",
    "\n",
    "def get_more_info_from_user(questions: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    In case of more infromation needed, this function asks user relavent info and gets you that info. \n",
    "\n",
    "    Args:\n",
    "        questions (list[str]): list of questions to ask \n",
    "\n",
    "    Returns:\n",
    "        str: answers for above questions from user\n",
    "    \"\"\"\n",
    "    print(\"------------------------\")\n",
    "    print(\"Please answer following questions\")\n",
    "    print(questions)\n",
    "    print(\"------------------------\")\n",
    "    answers = input()\n",
    "    return answers\n",
    "\n",
    "search_tool  = Tool(search_internet)\n",
    "get_more_info_from_user = Tool(get_more_info_from_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = OpenAIModel('gpt-4o-mini')\n",
    "entry_and_info_gathering = Agent(  \n",
    "    system_prompt='You are a expert in making decisions you make right decisions always. You will be given a situation and you have to ask questions to user to so that you can make a right decision which user asks. ',  \n",
    "    model = model, \n",
    "    tools = [search_tool, ask_more_info_from_user]\n",
    ")\n",
    "\n",
    "pros_identifier_agent  = Agent(\n",
    "    system_prompt = \"you have given a situation and some questions answered by user. You have to respond all the pros of that situation. \",\n",
    "    result_type=list[str], \n",
    "    model = model , \n",
    "    tools = [search_tool]\n",
    ")\n",
    "\n",
    "cons_identifier_agent = Agent(\n",
    "    system_prompt = \"you have given a situation and some questions answered by user. You have to respond all the cons of that situation. \",\n",
    "    result_type=list[str], \n",
    "    model = model,\n",
    "    tools = [search_tool]\n",
    ")\n",
    "\n",
    "conclusion_agent = Agent(\n",
    "    system_prompt = \"you have given a situation and some questions answered by user. Also the pros and cons about the situation you have provide conclusion for making the better decision. The highest pros and a decision which is ethical is usually the best decision, think and answer based on the knowledge and scenarios you've observed from past.\",\n",
    "    result_type=str,\n",
    "    model = model,\n",
    "    tools = [search_tool]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamedRunResult(_all_messages=[ModelRequest(parts=[SystemPromptPart(content='you have given a situation and some questions answered by user. You have to respond all the pros of that situation. ', dynamic_ref=None, part_kind='system-prompt'), UserPromptPart(content=\"I'm going through breakup what to do.\", timestamp=datetime.datetime(2025, 2, 9, 9, 37, 55, 715939, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[ToolCallPart(tool_name='get_more_info_from_user', args='{\"questions\":[\"How long was the relationship?\",\"How did the breakup happen?\",\"What are your current feelings about the breakup?\",\"Do you have a support system (friends, family) in place?\",\"What are you looking to achieve in moving on from this breakup?\"]}', tool_call_id='call_k9goltM6ONgxicilXssMsTHF', part_kind='tool-call')], model_name='gpt-4o-mini', timestamp=datetime.datetime(2025, 2, 9, 9, 37, 56, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='get_more_info_from_user', content=\"she's going through cancer she's gonna die in days \", tool_call_id='call_k9goltM6ONgxicilXssMsTHF', timestamp=datetime.datetime(2025, 2, 9, 9, 38, 6, 864129, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request')], _new_message_index=0, _usage_limits=UsageLimits(request_limit=50, request_tokens_limit=None, response_tokens_limit=None, total_tokens_limit=None), _stream_response=OpenAIStreamedResponse(_model_name='gpt-4o-mini', _usage=Usage(requests=0, request_tokens=None, response_tokens=None, total_tokens=None, details=None), _parts_manager=ModelResponsePartsManager(_parts=[ToolCallPart(tool_name='final_result', args='', tool_call_id='call_kW5gZxALSpY5bHLjjeZ6TwIZ', part_kind='tool-call')], _vendor_id_to_part_index={0: 0}), _event_iterator=<async_generator object OpenAIStreamedResponse._get_event_iterator at 0x116c8d470>, _response=<pydantic_ai._utils.PeekableAsyncStream object at 0x117a1fbc0>, _timestamp=datetime.datetime(2025, 2, 9, 9, 38, 7, tzinfo=datetime.timezone.utc)), _result_schema=ResultSchema(tools={'final_result': ResultTool(tool_def=ToolDefinition(name='final_result', description='The final response which ends this conversation', parameters_json_schema={'properties': {'response': {'items': {'type': 'string'}, 'title': 'Response', 'type': 'array'}}, 'required': ['response'], 'type': 'object'}, outer_typed_dict_key='response'), type_adapter=TypeAdapter(response_data_typed_dict))}, allow_text_result=False), _run_ctx=RunContext(deps=None, model=OpenAIModel(system_prompt_role=None), usage=Usage(requests=2, request_tokens=165, response_tokens=70, total_tokens=235, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0, 'cached_tokens': 0}), prompt=\"I'm going through breakup what to do.\", messages=[ModelRequest(parts=[SystemPromptPart(content='you have given a situation and some questions answered by user. You have to respond all the pros of that situation. ', dynamic_ref=None, part_kind='system-prompt'), UserPromptPart(content=\"I'm going through breakup what to do.\", timestamp=datetime.datetime(2025, 2, 9, 9, 37, 55, 715939, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[ToolCallPart(tool_name='get_more_info_from_user', args='{\"questions\":[\"How long was the relationship?\",\"How did the breakup happen?\",\"What are your current feelings about the breakup?\",\"Do you have a support system (friends, family) in place?\",\"What are you looking to achieve in moving on from this breakup?\"]}', tool_call_id='call_k9goltM6ONgxicilXssMsTHF', part_kind='tool-call')], model_name='gpt-4o-mini', timestamp=datetime.datetime(2025, 2, 9, 9, 37, 56, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='get_more_info_from_user', content=\"she's going through cancer she's gonna die in days \", tool_call_id='call_k9goltM6ONgxicilXssMsTHF', timestamp=datetime.datetime(2025, 2, 9, 9, 38, 6, 864129, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request')], tool_name=None, retry=0, run_step=2), _result_validators=[], _result_tool_name='final_result', _on_complete=<function _build_streamed_run_result.<locals>.on_complete at 0x117b1b560>, is_complete=False)\n"
     ]
    },
    {
     "ename": "UserError",
     "evalue": "stream_text() can only be used with text responses",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUserError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mstream_text():  \n\u001b[1;32m      7\u001b[0m             \u001b[38;5;28mprint\u001b[39m(message)\n\u001b[0;32m----> 9\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/code/decision_helper/.venv/lib/python3.12/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/Downloads/code/decision_helper/.venv/lib/python3.12/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py:202\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[53], line 6\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m pros_identifier_agent\u001b[38;5;241m.\u001b[39mrun_stream(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm going through breakup what to do.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      4\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m result:  \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mstream_text():  \n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(message)\n",
      "File \u001b[0;32m~/Downloads/code/decision_helper/.venv/lib/python3.12/site-packages/pydantic_ai/result.py:235\u001b[0m, in \u001b[0;36mStreamedRunResult.stream_text\u001b[0;34m(self, delta, debounce_by)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Stream the text result as an async iterable.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m!!! note\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m        performing validation as each token is received.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result_schema \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result_schema\u001b[38;5;241m.\u001b[39mallow_text_result:\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUserError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream_text() can only be used with text responses\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    237\u001b[0m usage_checking_stream \u001b[38;5;241m=\u001b[39m _get_usage_checking_stream_response(\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_usage_limits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musage\n\u001b[1;32m    239\u001b[0m )\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Define a \"merged\" version of the iterator that will yield items that have already been retrieved\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# and items that we receive while streaming. We define a dedicated async iterator for this so we can\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# pass the combined stream to the group_by_temporal function within `_stream_text_deltas` below.\u001b[39;00m\n",
      "\u001b[0;31mUserError\u001b[0m: stream_text() can only be used with text responses"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    async with pros_identifier_agent.run_stream(\n",
    "        \"I'm going through breakup what to do.\", \n",
    "    ) as result:  \n",
    "        print(result)\n",
    "        async for message in result.stream_text():  \n",
    "            print(message)\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:50:46.059 entry_and_info_gathering run prompt=I want to create AI project which people will use?.\n",
      "13:50:46.081   preparing model request params run_step=1\n",
      "13:50:46.082   model request\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Logfire</span> project URL: <a href=\"https://logfire.pydantic.dev/somesh/decision-maker\" target=\"_blank\"><span style=\"color: #008080; text-decoration-color: #008080; text-decoration: underline\">https://logfire.pydantic.dev/somesh/decision-maker</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mLogfire\u001b[0m project URL: \u001b]8;id=468619;https://logfire.pydantic.dev/somesh/decision-maker\u001b\\\u001b[4;36mhttps://logfire.pydantic.dev/somesh/decision-maker\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:50:49.251   handle model response\n",
      "13:50:49.252     running tools=['get_more_info_from_user']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    -----\n",
       "    Please answer the following questions\n",
       "    -----\n",
       "    ['What specific problem do you want your AI project to solve?', 'Who is your target audience or user base for \n",
       "this project?', 'What type of AI technology are you considering (e.g., machine learning, natural language \n",
       "processing, computer vision)?', 'What is your budget for this project?', 'Do you have any existing skills or \n",
       "resources that will help you in this project?']\n",
       "    -----\n",
       "    : </pre>\n"
      ],
      "text/plain": [
       "\n",
       "    -----\n",
       "    Please answer the following questions\n",
       "    -----\n",
       "    ['What specific problem do you want your AI project to solve?', 'Who is your target audience or user base for \n",
       "this project?', 'What type of AI technology are you considering (e.g., machine learning, natural language \n",
       "processing, computer vision)?', 'What is your budget for this project?', 'Do you have any existing skills or \n",
       "resources that will help you in this project?']\n",
       "    -----\n",
       "    : "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:51:04.254   preparing model request params run_step=2\n",
      "13:51:04.255   model request\n",
      "13:51:08.699   handle model response\n",
      "13:51:08.699     running tools=['transfer_to_pros_agent', 'transfer_to_cons_agent']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Getting pros for: AI project for the general public using Python and transformers\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Getting pros for: AI project for the general public using Python and transformers\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:51:08.707     pros_identifier_agent run prompt=Please evaluate the topic and provide pros for this AI project...answer AGI, for all, Python, transformers, a billion dollars.."
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Getting cons for: AI project for the general public using Python and transformers\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Getting cons for: AI project for the general public using Python and transformers\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13:51:08.708       preparing model request params run_step=1\n",
      "13:51:08.709       model request\n",
      "13:51:08.715     cons_identifier_agent run prompt=Please evaluate the topic and provide cons for this AI project...answer AGI, for all, Python, transformers, a billion dollars..\n",
      "13:51:08.715       preparing model request params run_step=1\n",
      "13:51:08.716       model request\n",
      "                 pros_identifier_agent run prompt=Please evaluate the topic and provide pros for this AI project...answer AGI, for all, Python, transformers, a billion dollars..\n",
      "13:51:12.782       handle model response\n",
      "                 cons_identifier_agent run prompt=Please evaluate the topic and provide cons for this AI project...answer AGI, for all, Python, transformers, a billion dollars..\n",
      "13:51:22.946       handle model response\n",
      "13:51:22.958   preparing model request params run_step=3\n",
      "13:51:22.958   model request\n",
      "13:51:27.448   handle model response\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">InfoGatheringOutput</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">query</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'AI project for the general public using Python and transformers'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">questions</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'What potential benefits can this project offer to users?'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'How can it improve their daily lives?'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'What market demand exists for this type of AI application?'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'What are the potential downsides or challenges of this project?'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'What risks are associated with developing AGI?'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'How competitive is the market for similar AI projects?'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">answers_from_user</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'AGI, for all, Python, transformers, a billion dollars.'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">pros</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Accessibility of AGI tools for everyone'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Potential for significant cost savings in various sectors'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Enhancement of productivity and efficiency in daily tasks'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Facilitation of learning and education through AI assistance'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Opportunities for innovation in technology and business'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">cons</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Development of AGI raises ethical concerns regarding control and alignment with human values.'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'High financial investment of a billion dollars increases the pressure for success, which may lead to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">compromised safety and quality.'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Technical challenges in developing robust and reliable AGI could lead to significant delays and increased </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">costs.'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Risk of bias and discrimination in AI models if data used for training is not carefully managed.'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Market competition may lead to rushed development, resulting in incomplete or flawed systems being </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">deployed to the public.'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mInfoGatheringOutput\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mquery\u001b[0m=\u001b[32m'AI project for the general public using Python and transformers'\u001b[0m,\n",
       "    \u001b[33mquestions\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[32m'What potential benefits can this project offer to users?'\u001b[0m,\n",
       "        \u001b[32m'How can it improve their daily lives?'\u001b[0m,\n",
       "        \u001b[32m'What market demand exists for this type of AI application?'\u001b[0m,\n",
       "        \u001b[32m'What are the potential downsides or challenges of this project?'\u001b[0m,\n",
       "        \u001b[32m'What risks are associated with developing AGI?'\u001b[0m,\n",
       "        \u001b[32m'How competitive is the market for similar AI projects?'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33manswers_from_user\u001b[0m=\u001b[32m'AGI, for all, Python, transformers, a billion dollars.'\u001b[0m,\n",
       "    \u001b[33mpros\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[32m'Accessibility of AGI tools for everyone'\u001b[0m,\n",
       "        \u001b[32m'Potential for significant cost savings in various sectors'\u001b[0m,\n",
       "        \u001b[32m'Enhancement of productivity and efficiency in daily tasks'\u001b[0m,\n",
       "        \u001b[32m'Facilitation of learning and education through AI assistance'\u001b[0m,\n",
       "        \u001b[32m'Opportunities for innovation in technology and business'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mcons\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[32m'Development of AGI raises ethical concerns regarding control and alignment with human values.'\u001b[0m,\n",
       "        \u001b[32m'High financial investment of a billion dollars increases the pressure for success, which may lead to \u001b[0m\n",
       "\u001b[32mcompromised safety and quality.'\u001b[0m,\n",
       "        \u001b[32m'Technical challenges in developing robust and reliable AGI could lead to significant delays and increased \u001b[0m\n",
       "\u001b[32mcosts.'\u001b[0m,\n",
       "        \u001b[32m'Risk of bias and discrimination in AI models if data used for training is not carefully managed.'\u001b[0m,\n",
       "        \u001b[32m'Market competition may lead to rushed development, resulting in incomplete or flawed systems being \u001b[0m\n",
       "\u001b[32mdeployed to the public.'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## What do we have to make \n",
    "# \n",
    "# user will give you a situation. \n",
    "# first agent will ask the additional questions if necessary \n",
    "# \n",
    "# second and third agent will provide pros and cons of the situation. If it requires any tool calls have a web search and simple calculator tool powered by python make both tools availalbe to this agent. \n",
    "# \n",
    "# forth agent: \n",
    "# will provide the final conclusion if user should take that decision or not. \n",
    "\n",
    "# %% [markdown]\n",
    "# ## Imports\n",
    "\n",
    "# %%\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import BaseModel\n",
    "from rich import print\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "import asyncio\n",
    "import logfire\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "logfire.configure()\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from duckduckgo_search import DDGS\n",
    "from rich.prompt import Prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Tools for internet search \n",
    "\n",
    "# %%\n",
    "from pydantic_ai.tools import Tool\n",
    "\n",
    "def search_internet(query: str) -> list[dict]: \n",
    "    \"\"\"Searches the web for given query and returns top 5 results\n",
    "\n",
    "    Args:\n",
    "        query (str): Query to search\n",
    "\n",
    "    Returns:\n",
    "        list: List of top 5 results\n",
    "    \"\"\"\n",
    "    results = DDGS().text(query, max_results=5)\n",
    "    return results\n",
    "\n",
    "async def get_more_info_from_user(questions: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    In case of more infromation needed, this function asks user relavent info and gets you that info. \n",
    "\n",
    "    Args:\n",
    "        questions (list[str]): list of questions to ask \n",
    "\n",
    "    Returns:\n",
    "        str: answers for above questions from user\n",
    "    \"\"\"\n",
    "    answers = Prompt.ask(f\"\"\"\n",
    "    -----\n",
    "    Please answer the following questions\n",
    "    -----\n",
    "    {questions}\n",
    "    -----\n",
    "    \"\"\")\n",
    "    return answers\n",
    "\n",
    "search_tool  = Tool(search_internet)\n",
    "get_more_info_from_user = Tool(get_more_info_from_user)\n",
    "\n",
    "\n",
    "# %%\n",
    "class InfoGatheringOutput(BaseModel): \n",
    "    \"\"\"The model returned by info gathering agent.\n",
    "\n",
    "    Args:\n",
    "        BaseModel (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    query: str \n",
    "    questions: list[str]\n",
    "    answers_from_user: str \n",
    "\n",
    "class ProsOutput(BaseModel): \n",
    "    \"\"\"\n",
    "    The model returned by pros agent.\n",
    "    Args:\n",
    "        BaseModel (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    pros: list[str]\n",
    "\n",
    "class ConsOutput(BaseModel): \n",
    "    \"\"\"\n",
    "    The model returned by cons agent.\n",
    "\n",
    "    Args:\n",
    "        BaseModel (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    cons: list[str]\n",
    "\n",
    "@dataclass\n",
    "class Deps:\n",
    "    pros_identifier_agent: Agent\n",
    "    cons_identifier_agent: Agent\n",
    "    conclusion_agent: Agent\n",
    "\n",
    "\n",
    "model = OpenAIModel('gpt-4o-mini')\n",
    "entry_and_info_gathering = Agent(  \n",
    "    system_prompt='You are a expert in making decisions you make right decisions always. You will be given a situation and you have to ask questions to user to so that you can make a right decision which user asks. You have access to the tool call for the getting more info from user please use that, after gathering you can use pros and cons gathering agent to get pros and cons. ',  \n",
    "    model = model, \n",
    "    tools = [search_tool, get_more_info_from_user],\n",
    "    result_type=InfoGatheringOutput\n",
    ")\n",
    "\n",
    "\n",
    "pros_identifier_agent  = Agent(\n",
    "    system_prompt = \"you have given a situation and some questions answered by user. You have to respond all the pros of that situation. \",\n",
    "    result_type=ProsOutput, \n",
    "    model = model, \n",
    "    tools = [search_tool]\n",
    ")\n",
    "\n",
    "cons_identifier_agent = Agent(\n",
    "    system_prompt = \"You are the best critic which evaluates situation and gives critical responses which are importatnt to make decision. you have given a situation and some questions answered by user. You have to respond all the cons of that situation. \",\n",
    "    result_type=ConsOutput, \n",
    "    model = model,\n",
    "    tools = [search_tool]\n",
    ")\n",
    "\n",
    "conclusion_agent = Agent(\n",
    "    system_prompt = \"you have given a situation and some questions answered by user. Also the pros and cons about the situation you have provide conclusion for making the better decision. The highest pros and a decision which is ethical is usually the best decision, think and answer based on the knowledge and scenarios you've observed from past.\",\n",
    "    result_type=str,\n",
    "    model = model,\n",
    "    tools = [search_tool]\n",
    ")\n",
    "\n",
    "@entry_and_info_gathering.tool\n",
    "def transfer_to_pros_agent(ctx: RunContext, query: str, questions: list[str], answer: str ) -> str:\n",
    "    print(f\"Getting pros for: {query}\")\n",
    "    result = ctx.deps.pros_identifier_agent.run_sync(f\"Please evaluate the topic and provide pros for this {query} previous agent asked this question to the user {questions} and user provided following answer {answer}.\")\n",
    "    return result\n",
    "\n",
    "@entry_and_info_gathering.tool\n",
    "def transfer_to_cons_agent(ctx: RunContext, query: str, questions: list[str], answer: str ) -> str:\n",
    "    print(f\"Getting cons for: {query}\")\n",
    "    result = ctx.deps.cons_identifier_agent.run_sync(f\"Please evaluate the topic and provide cons for this {query} previous agent asked this question to the user {questions} and user provided following answer {answer}.\")\n",
    "    return result\n",
    "\n",
    "@pros_identifier_agent.tool\n",
    "@cons_identifier_agent.tool\n",
    "def transfer_to_conclusion_agent(ctx: RunContext, query: str, pros: list[str], cons: list[str] ) -> str:\n",
    "    print(f\"Getting conclusion for: {query}\")\n",
    "    result = ctx.deps.conclusion_agent.run_sync(f\"Evalute the pros {pros} and cons{cons} should we do this {query}\")\n",
    "    return result \n",
    "\n",
    "deps = Deps(pros_identifier_agent = pros_identifier_agent, cons_identifier_agent = cons_identifier_agent, conclusion_agent = conclusion_agent)\n",
    "\n",
    "# %%\n",
    "async def main(query: str):\n",
    "    res =await entry_and_info_gathering.run(query, deps = deps)\n",
    "    print(res.data)\n",
    "    return res \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main(query = \"I want to create AI project which people will use?.\"))\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
